\chapter{Event modelling}
\label{EventModelling}

\begin{flushright}{\slshape    
[Artifacts] mediate activity that connects a person not only with \\
 the world of objects, but also with other people. This means \\
that a person's activity assimilates the experience of humanity.} \\ \medskip
    ---  Aleksei N. Leontiev, founder of activity theory
\end{flushright}

\marginpar{Parts of this chapter appear in \cite{Niezen2011}.}

Events can be used to associate occurrences with people, places and objects at a specific time instant or during a time interval. In the previous chapter we looked at how objects can be modelled. The focus of this chapter is how the occurrence and its associated time instant or interval can be modelled. The modelling of people and places is considered to be outside the scope of this work.

%- Event ontology
%- Music ontology (uses Events ontology, also see 15/02/11)

\marginpar{Interaction events were first introduced in Section \ref{OntologyDesign1}.}
Interaction events are generated when a user interacts with a smart object that is connected to another smart object using semantic connections. Interaction events are high-level input events which report the intention of the user's action directly, rather than just reporting the associated hardware input event that triggered the action. This high level of abstraction enables developers to write applications which will work across different devices and services, without having to write specific code for each possible input device. The W3C Web Events Working Group defined four conceptual layers for interactions (for touch- and pen-tablet interaction) \cite{w3cevents}:

\begin{description}
\item [physical] This is lowest level, and deals with the physical actions that a user takes when interacting with a device, such as pressing a physical button.
\item [gestural] This is a middle layer between the physical and intentional, and describes specific mappings between the two; for example, a ``pinch'' gesture may represent the user placing two fingers on a screen and moving them together at the physical layer. This may map to a ``zoom-in'' event at the intentional layer.
\item [representational] This is the highest level of abstraction in the event model, and indicates the means by which the user is performing a task, such as zooming in, panning, navigating to the next page, activating a control, etc.
\item [intentional] This layer indicates the intention of the task a user is trying to perform, such as viewing more or less detail (zooming in and out), viewing another part of the larger picture (panning), and so forth.
\end{description}

\begin{table}
\centering
\begin{tabular}{|l|l|}
\hline
Interaction Event & Entity this event can be performed on\\
\hline
AdjustLevelEvent & Volume, Lighting \\
switchOnEvent & Lighting, any SmartObject \\
NavigateEvent & Playlist, Menu, SequentialData \\
UndoEvent & Any other interaction event \\
StopEvent & Application, Media \\
%DragAndDrop & Media \\
%Query & Media, other events \\
\hline
\end{tabular}
\caption{Examples of interaction events in a smart environment}
\label{transformationTable}
\end{table}

In Table \ref{transformationTable} examples of possible interaction events are shown, together with possible entities associated with these events. They exist at different interaction layers, mostly at the representational and intentional level.

%An interaction event has more than one possible entity that it can be performed on, as well as more than one possible entity that triggers it, i.e., there exists some ambiguity. Only when no ambiguity exists as to which entity it is performed on, as well as the action which the user is trying to accomplish, we refer to it as an intentional event. Semantic transformations occur between physical actions (such as pressing a button or doing a gesture) and representational events, as well as between representational events and intentional events.


%end S3E


In \ac{OWL}, there are are two approaches to modelling time:

\begin{itemize}
	\item Using datatype properties: Event instances can be related to a literal with a \ac{XSD} datatype such as \texttt{xsd:date} or \texttt{xsd:dateTime}. This is the approach that we used and is described later in this chapter.
	\item Using object properties: Classes are used to define temporal intervals, and event instances are linked to instances of these classes using object properties.
\end{itemize}


\section{Related work}
We now look at various existing event ontologies that we build upon to model interaction events in ubiquitous computing environments.

\subsection{The Event Ontology}
The \ac{EO}\footnote{http://motools.sf.net/event/event.html} was developed within the context of the Music ontology\footnote{http://purl.org/ontology/mo/}. Although originally created to describe musical performances and events, it is currently the most commonly used event ontology in the Linked Data community \cite{Shaw2009}.

The Timeline\footnote{http://motools.sf.net/timeline/timeline.html} ontology, used to define time instants and intervals, also performs part of this collection of ontologies. Reasoning with temporal information is discussed further in Section \ref{TemporalReasoning}.  

\subsection{DUL}

The \ac{DUL} upper ontology is a lightweight version of the \ac{DOLCE} ontology. \ac{DUL} defines the class \texttt{Event} next to the disjoint upper classes \texttt{Object}, \texttt{Abstract} and \texttt{Quality} \cite{Scherp2011}.

\ac{DUL} allows for both the approaches to modelling time with \ac{OWL}, either with the \texttt{hasEventDate} datatype property, or with a \texttt{TimeInterval} class and the \texttt{isObservableAt} object property.

Events can either be related to a \texttt{Place} with the \texttt{hasLocation} property, or to a \texttt{SpaceRegion} with the \texttt{hasRegion} property (where \texttt{SpaceRegion} resolves to a geospatial coordinate system).

\ac{DUL} uses a \texttt{hasParticipant} property to relate an event to an object. To link events to sub-events, it uses the \texttt{hasPart} property.

\subsection{Event-Model-F}

The Event-Model-F ontology builds upon the \ac{DUL} ontology to describe events. To describe the participation of an object in an event, the Event-Model-F ontology uses the \ac{DnS} ontology design pattern \cite{Shaw2009}.\marginpar{The \ac{DnS} pattern is described in more detail in Chapter \ref{OntologyEngineering}.} 

An object is defined as a \texttt{Participant}, where \texttt{LocationParameter} is used to describe the general spatial region of the object \cite{Scherp2011}. \texttt{TimeParameter} describes the general temporal region when the event happened by parametrizing a \texttt{DUL:TimeInterval}. A composite event \texttt{Composite} is composed out of a number of \texttt{Component}s.

\subsection{Linked Open Descriptions of Events (LODE)}

The \ac{LODE} ontology\footnote{http://linkedevents.org/ontology/} is an ontology for publishing descriptions of historical events as Linked Data. It builds upon the work of the previous ontologies described in this section, in order to improve interoperability with legacy event collections. Its \texttt{Event} class is directly equivalent to those defined by \ac{EO} and \ac{DUL}. 

It uses time intervals to link events to ranges of time, where its \texttt{atTime} property is a sub-property of the \ac{DUL} \texttt{isObservableAt} property. There is also a distinction between places and spaces, where a \texttt{inSpace} property relates the event to a space and a \texttt{atPlace} is a sub-property of the \ac{DUL} \texttt{hasLocation} property.


\subsection{Ontologies for temporal reasoning}
\label{TemporalReasoning}

Temporal reasoning is important when we want to work with time intervals, for example when using Allen's Interval Algebra to define temporal relations between events. Allen's Interval Algebra is also used by the \ac{DOLCE} \cite{Scherp2011} upper ontology. \marginpar{The \ac{DOLCE} upper ontology is discussed in more detail in Chapter \ref{OntologyEngineering}.}

\ac{SWRL} Basic Temporal Built-ins support \texttt{xsd:date} and \texttt{xsd:dateTime} with Allen's Interval Algebra. The Advanced Temporal Built-ins uses the \texttt{temporal} ontology \cite{OConnor2010} to provide additional functionality, for example having different granularity levels.


The \ac{DC} Terms ontology has a \texttt{temporal} property to describe temporal coverage of a resource with a range \texttt{periodOfTime}.

TopBraid Composer has a Calendar ontology that defines an \texttt{Event} and its \texttt{startTime} and \texttt{endTime} (as \texttt{xsd:dateTime}). This can then be used with the Calendar View widget in the editor. 

In \ac{SPARQL}, we can use the $<$ and $>$ operators on dates, e.g. \mint{sparql}|FILTER(?date > "2005-10-20T15:31:30"^^xsd:dateTime)|

Using \ac{SPIN}, can also cast a \texttt{xsd:dateTime} value to a string using the \texttt{fn:substring} function, e.g. \mint{sparql}|fn:substring(xsd:string(afn:now()),0,10)|




\section{User-action events}
%begin S3E









\section{Interaction events}
An interaction event happens at a specific time, is generated by a smart object and has an optional data value associated with the event.

\begin{figure}[bth]
        \includegraphics[width=\linewidth]{interactionEvent}
        \caption{An interaction event as modelled in the ontology}
        \label{interactionEvent}
\end{figure}

An example of an event generated when an alarm is set is
\begin{minted}{turtle}
ie:event-43495d51-29e3-11b2-807e-ac78eefc1f82 
	rdf:type ie:AlarmSetEvent ;
	ie:generatedBy ie:phone1 ;
	ie:inXSDDateTime "2012-01-17T11:22:06.887+01:00"^^xsd:dateTime ;
	ie:dataValue "2012-01-17T12:00:00+01:00"^^xsd:dateTime .
\end{minted}

A mapping between our interaction event model and the other event ontologies is shown in Table \ref{eventMappings}.

\begin{table}
    \myfloatalign
  \begin{tabularx}{\textwidth}{llll} 
	\toprule
    \tableheadline{\ac{DUL}} & \tableheadline{EO} & \tableheadline{LODE} & \tableheadline{Interaction Events}\\ 
    \midrule

isObservableAt & time & atTime & inXSDDateTime \\
 & place & inSpace & \\
hasLocation & & atPlace & \\
hasParticipant & factor & involved & \\
involvesAgent & agent & involvedAgent & generatedBy \\
	
    \bottomrule
  \end{tabularx}
  \caption{Mappings between the various event models [modified from \cite{Shaw2009}]}\label{eventMappings}
\end{table}



The \texttt{duration} property is used to define the length of event. For example to increase the brightness of a lamp, we can generate an event to increase a value to a set maximum over a time period:

\begin{minted}{turtle}
ie:event-43495d51-29e3-11b2-807e-ac78eefc1f83 
	rdf:type ie:IncreaseLevelEvent ;
	ie:generatedBy ie:wakeup1 ;
	ie:inXSDDateTime "2012-01-17T11:23:06.887+01:00"^^xsd:dateTime ;
	ie:dataValue 255 .
	ie: duration "PT3S"^^xsd:duration .
\end{minted}


The advantage of linking event instances directly with dates is simplicity. There are fewer abstractions to deal with, and it is easier to sort events chronologically and compare them \cite{Shaw2009}. 


We distinguish between \emph{control} and \emph{content}. Interacting with a device, e.g. pressing a ``Play'' button or moving a volume slider, is considered control and described using interaction events. Content, e.g. a song or a photo stored on the device,  is referred to by where it exists on the device, as well as how it can be rendered using the media capabilities of the device. We consider interaction events to be traceable, reversible and identifiable. Each interaction event has an associated timestamp and a unique ID that is generated when the event occurs. 

\marginpar{Intentional, incidental and expected interactions were introduced in Section \ref{intentionalSpectrum}.}
An intentional interaction, like pressing a light switch, is an interaction event if the light switch shares this information with other devices. Incidental or expected interactions, like the light turning on if the presence sensor is triggered, are also interaction events. System events, like a \texttt{TimeSetEvent}, which are invisible to the user are not considered interaction events.


In the iStuff toolkit \cite{Ballagas2003} \emph{hierarchical event} structures were used to abstract low-level events into application-level events. \marginpar{Also see the related work on task models in Section \ref{interactionTasks}.} We also introduce the notion of an event hierarchy, where low-level events are considered to be very \emph{generic}, as they do not report a user's intention directly \cite{Niezen2011}. These low-level events first need to be transformed into intentional events---events that express user intention.

%some of this might need to move to, or be repeated in the section where we describe our interaction model.
%\marginpar{Nielsen's virtual protocol model and its interaction levels were discussed in Section \ref{nielsenVPM}.}
We build on the different interaction levels/layers introduced in the related work of Section \ref{InteractionModels} to categorise interaction events. As an example, consider the case where a rocker switch, modelled as an interaction primitive on a mobile device, is used to control the volume of music in a room. One could start modelling the interaction on the physical level with a \texttt{ButtonEvent}, but it would be more meaningful to model it on the lexical level as a \texttt{ButtonUpEvent}. On the syntax level this event could increment a quantity by one, while an event generated by a volume dial might have a discrete value attached to it. When this is combined with other device information, for example that the device is being used to stream music to the environment, we can infer on the semantic level that it is a \texttt{VolumeUpEvent}. When this is combined with other contextual information, for example that the device is currently connected to a speaker system in the same room, we can even infer on the task level that the music volume in the room should be set to a specific value with a \texttt{MusicVolumeUpEvent}, to which all connected devices can respond. We acknowledge that in most cases it may not be possible to make inferences on the goal level, which in this example could be the user's intent to set the music volume in the room to a level that is loud enough for everyone in the room to dance to.

\begin{figure}[bth]
	\digraph[scale=0.45]{InteractionModelToOntology}{
		{rank=same; Semantic [label="Semantic Level", shape=plaintext]; AdjustLevelEvent; }
		{rank=same; Syntactic [label="Syntactic Level", shape=plaintext]; VolumeEvent; SmartObject;}
		{rank=same; Lexical [label="Lexical Level", shape=plaintext];}
		{rank=same; Alphabetic [label="Alphabetic Level", shape=plaintext]; ButtonEvent; }
		Semantic->Syntactic->Lexical->Alphabetic [style="invis"];
		VolumeEvent -> AdjustLevelEvent [label="is-a"];
		VolumeEvent -> SmartObject [label="generatedBy"];
		AdjustLevelEvent -> SmartObject [label="generatedBy", style="dashed"];
	}
	\caption{How the interaction model levels relate to the ontology}
	\label{InteractionModelToOntology}        
\end{figure}

Figure \ref{InteractionModelToOntology} shows how the interaction model levels relate to the concepts in the ontology that was developed through the various iterations. A \texttt{ButtonEvent} describes a user interaction on the alphabetical level, but carries very little meaning. For example, was the button switched up or down, or was it pressed? If we know that the button was switched up, we have a lexical token that describes the interaction, but it still needs to be combined with other contextual information to determine the user's intention. On the other hand, if the interaction is described as a \texttt{VolumeEvent}, the user's intention is described on a syntactic level, and it becomes possible to map the event to a set of predefined semantic events, for example an \texttt{AdjustLevelEvent}. Using semantic reasoning, we can also infer that this \texttt{AdjustLevelEvent} was generated by the same smart object.




\subsection{System events}
\label{SystemEvents}
When a smart object first subscribes to the smart space, it only listens for events that are generated by other smart objects connected to it. This means that we also need some way of distributing system-wide events that all devices listen to. As an example, consider the \texttt{TimeSetEvent}. When the user sets the time on one device, we want the time to be immediately updated on all the other smart objects in the smart space, even if they are not connected to the device that generated the \texttt{TimeSetEvent}. If we define \texttt{TimeSetEvents} as a subset of \texttt{SystemEvents}, each smart object only need to subscribe to events of type \texttt{SystemEvent}.


\subsection{Feedback}
When setting an alarm for example, augmented feedback should be provided on all devices. Functional feedback (the alarm sound when alarm fires) is delayed, so augment functional feedforward should be provided. We thus define two types of feedback events:

\begin{itemize}
\item \texttt{PreviewEvent} - generated when a possible connection is being explored, displaying the possible functionalities enabled by the connection, i.e. augmented functional feedforward.
\item \texttt{IndicatorEvent} - augmented feedback when smart object is connected and there is no immediate functional feedback, e.g. a sink ``beeping'' when the alarm is set on the source; used to confirm actions.   
\end{itemize} 

The type of feedback required depends on the functionality of the connection. It is important for the feedback to coincide in time and modality with the event generated, as to maintain the causal link that is perceived by the user.

The device used to make the connection (in our test setup - the Connector object) creates a temporary connection to the devices to be connected in order to generate a \texttt{PreviewEvent}. This \texttt{tempConnectedTo} property is a sub-property of the \texttt{connectedTo} property. This means that the smart objects will handle it as if it is a regular connection, and when the Connector object removes the \texttt{tempConnectedTo} relationship, the inferred \texttt{connectedTo} relationship will disappear as well.




===Event modelling
To model an \texttt{AlarmSetEvent}, the following restriction was used:\\

\noindent
\texttt{AlarmSetEvent}~$\models$~\texttt{SetEvent}~$\sqcap$~\texttt{dataValue}~$\forall$~\texttt{xsd:dateTime}~$\sqcap$~\texttt{dataValue}~$= 1$\\

This restriction will not infer that an event is an \texttt{AlarmSetEvent}, but it will restrict the descriptions of these events to what can actually be described.





\subsection{Task Continuity}

Tungare et al. \cite{Tungare2007} defined a \emph{task disconnect} as ``the break in continuity that occurs due to the extra actions outside the task at hand that are necessary when a user attempts to accomplish a task using more than one device.'' \cite{Kuniavsky} states that consistency is the key aspect in creating task continuity across devices, and that \emph{interaction vocabularies} have recently emerged as a way of consistently interacting with a range of devices. This ranges from simple vocabularies of light patterns and motion as used by the (now discontinued) Nabaztag Internet-connected rabbit that could compose ``sentences'' with more complex meaning, to a set of visual icons by Timo Arnall \cite{Arnall2006} that represents various kinds of touch-based RFID interactions.
These interaction vocabularies try to smooth over task disconnects through consistency.

TODO: Valkkynen2010,  Rekimoto et al. (Ayatsuka2003), Nordby2010

TODO: Select subset of icons from % http://www.elasticspace.com/presentations/graphic_language_touch_rfid_nfc.pdf

TODO: How we developed a vocabulary for semantic connections

TODO: Creating a vocabulary of interaction events (based on existing task models)
