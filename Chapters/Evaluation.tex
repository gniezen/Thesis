\chapter{Evaluation}
\label{Evaluation}

\section{Evaluating the system performance of the software architecture}
\label{performance}
%SeNAmI start
\subsection{Related work}
The two M3-based smart space implementations described in Section \ref{m3} were evaluated by Etel\"aper\"a et al \cite{Etelapera2011}. They performed both a qualitative evaluation and quantitative measurements. The performance measurements were made on a Intel Atom 1.6GHz laptop connected via a 100Mbps Ethernet router to a Intel Pentium M 1.7GHz laptop. The qualitative evaluation focused on documentation, installation process and portability as well as run-time usability. According to \cite{Etelapera2011} RIBS is up to 237 times faster than Smart-M3 in certain instances, but that its memory model limits the number of use cases it can be applied to. RIBS uses static memory allocation (with no disk storage) and a bitcube triple store, which means that the maximum number of triples have to be known a priori.

Query time measurements for Smart-M3 indicated a query time of 4.4ms for one triple and 8.6ms for 10 triples. For RIBS a query time of 0.65ms were measured for one triple. RIBS did not support querying 10 triples at the time the evaluation was performed. Subscription time measurements indicated a subscription indication time of 140ms for Smart-M3, while RIBS measured 0.75ms.

\cite{Bhardwaj2011} did a performance analysis based on end-to-end delay measurements between the smart objects in smart spaces. The analysis shows that the end-to-end delays are mostly dominated by KP-to-SIB updates, rather than the processing delays on KPs or on the SIB.

% A number of ontologies have been developed for ubiquitous computing environments. Chen et al. \cite{chen2004} defined SOUPA, a context ontology based on OWL (Web Ontology Language), to support ubiquitous agents in their Context Broker Architecture (CoBrA). The ontology supports describing devices on a very basic level (e.g. typical object properties are \texttt{bluetoothMAC} or \texttt{modelNumber}), but it has no explicit support for modeling more general device capabilities.

% Ngo et al. \cite{Ngo2004} developed the CAMUS ontology in OWL to support context awareness in ubiquitous environments. Their device ontology is based on the FIPA device ontology specification\footnote{http://www.fipa.org/specs/fipa00091/SI00091E.html}. Unfortunately it does not define a notion of completeness, and the ontology is thus not considered generic enough for general use in ubicomp environments.

Luukkala et al \cite{Luukkala2010} used Smart-M3 with Answer Set Programming (ASP) techniques to handle resource allocation and conflict resolution. They used the SPICE Mobile Ontology\footnote{http://ontology.ist-spice.org/} to describe device capabilities and ASP as a rule-based approach to reasoning. The SPICE ontology allows for the definition of device capabilities in a sub-ontology called Distributed Communication Sphere (DCS) \cite{Villalonga2009}. %While the ontology provides for a detailed description of the different modality capabilities, e.g. being able to describe force feedback as a \texttt{TactileOutputModalityCapability}, there are no subclass assertions made for other device capabilities.

%This could be extended with work from Riboni et al; Liming Chen (and maybe Sabou's paper)

\subsection{Implementation}
\label{implementation}

The software framework that was used to implement the system is based on the SOFIA IOP. SOFIA\footnote{http://www.sofia-project.eu/} (Smart Objects For Intelligent Applications) is an European research project within the ARTEMIS framework that attempts to make information in the physical world available for smart services - connecting the physical world with the information world. The SOFIA IOP uses a blackboard architectural model that implements the ideas of space-based computing \cite{Honkola2010}. It consists of two main components: a SIB (Semantic Information Broker) that acts as a common, semantic-oriented store of information and device capabilities, and KPs (Knowledge Processors), virtual and physical smart objects that interact with one another through the SIB. Various SIB implementations exist that conform to the M3 specification, of which Smart-M3 was the first open source reference implementation released in 2009\footnote{http://sourceforge.net/projects/smart-m3/}. RIBS (RDF Information Base System) is a C-based implementation of M3 targeted for devices with low processing power, but requires a high amount of memory \cite{Etelapera2011}.  The SIB implementation used in the pilot is called ADK-SIB (Application Development Kit SIB) and was developed within the SOFIA project. 

%The system architecture of the implementation of the smart home pilot is shown in Figure \ref{pilot}.
The smart home pilot follows the following scenario:

\textit{Mark and Dries enter their home. The intelligent lighting system (Presence KP) detects their presence, and switches the lights on, and notifies the smart space about user presence. The decorative wall-wash lights (Lamp KP) are in turn notified of user presence by the smart space, and turn themselves on. Mark and Dries start listening to music (Music Player KP). They would like to try to render the music on a lighting device to also create some visual effects accompanying the music. They query the smart space and find out that the lighting device can render these light effects (using the Sound/Light Transformer KP). They make a connection between the music player and the lighting device using the Connector (Connector KP). The light is rendered on the lighting device. To put the focus on the lighting device, the decorative wall-wash lights in the room automatically dim themselves down. At the same time, the light pattern also starts being rendered on the remote lighting device, where Mark's sister Sofia can observe the same light effects in her own house.}

\textit{At another location: after a while, Sofia is curious and wants to listen to the music that Mark and Dries are listening to. She connects her lighting device to her stereo, and the same song plays on her surround sound system (remote Music Player KP).}
% --Note: Since we don't mention spotlight navigation elsewhere, Sachin suggested we remove it from the scenario

%She uses the spotlight navigation device to make a connection from the bonding device to the stereo. When she starts using the spotlight navigator, the lights in the room dim down to enhance the visibility of the spotlight. Once the connection is made, the lights go back up.


% \begin{figure}
% \centering
% \includegraphics[width=250px]{pilot}
% \caption{System architecture of the Smart Home Pilot}
% \label{pilot}
% \end{figure}

In the smart home pilot, media content is shared among several devices in a smart home setting. Music is shared between a mobile device, a stereo speaker set and a lighting device that renders the mood of the music with colored lighting. The music experience, consisting of both light and music information, is also shared remotely between friends living in separate homes through the lighting device. Other lighting sources, like the smart functional lighting and the smart wall wash lights are sensitive to user presence and the use of other lighting sources in the environment.

The performance measurements were made in an environment that approximates a real-world home environment for these kinds of devices. Two wireless routers were placed in two different locations (Home A and B), bridged with an ethernet network cable. One router was configured to act as a DHCP server, while the other acted as a network bridge. The Sound/Light Transformer (SLT) KP was connected to the router in Home B, while the Connector KP, Music Player KP and SIB were connected to the router in Home A. All components were connected to the network via the 802.11g wireless protocol. The system specifications of each component used in the performance evaluation is shown in Table \ref{specs}.

\begin{table*}[!t]
\caption{System specifications of components used in measurement evaluation}
\label{specs}
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
Component & CPU & Operating system & Memory & Programming Language \\
\hline
Semantic Information Broker (SIB) & Intel Core 2 Duo 2.8GHz & Ubuntu Linux 10.04 & 4GB & Java \\
Sound/Light Transformer KP & Intel Core 2 Duo 2.2GHz & Ubuntu Linux 11.04 & 2GB & Java\\
Connector KP & Intel Core 2 Duo 2.6GHz & Mac OS X 10.6.8 & 4GB & Python\\
Music Player KP & OMAP 3430 ARM Cortex-A8 & Maemo Linux 5 & 256MB & Python\\
Presence KP  & Intel Pentium M & Ubuntu Linux 10.04 & 512MB & Python\\
Lamp KP  & Intel Pentium M & Ubuntu Linux 10.04 & 512MB & Python\\
\hline
\end{tabular}
\end{table*}

Figure \ref{soundLight} and Figure \ref{connectorseq} show the sequence diagrams of the measurements made for the SLT KP and the Connector KP respectively. During the pilot, 86 measurements were made by the SLT KP (each time an event was received), and 961 measurements were made by the Connector KP (each time a user scans a tag).

\begin{figure}
\centering
\includegraphics[width=150px]{soundLight}
\caption{Sequence diagram of Sound/Light Transformer KP query measurement}
\label{soundLight}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=150px]{connector}
\caption{Sequence diagram of Connector KP query measurement}
\label{connectorseq}
\end{figure}

For the music player KP, we measured the time between inserting a new event, and receiving an update from the SIB indicating that the specific event had occurred. First a subscription is made to the \texttt{PlayEvent} type, as seen in Figure \ref{N900}. A new \texttt{PlayEvent} is generated by the KP, and when the KP is notified of this event by the SIB, the KP queries the SIB to determine if the notification is indeed for the event that it generated itself.

The Lamp-KP was connected to the decorative wall-wash lights (four LED lamps), creating colored illumination on the wall of the room. The lamps are shown in Figure \ref{lamp-KP}, including a description of its components. The Presence KP (P-KP) determines the presence of a user in an activity area of a room and sends the presence information to the SIB. The Lamp-KP is subscribed to this presence information, and gets updated whenever the presence is updated by the P-KP to the SIB. There are two states to be updated by the P-KP on SIB: \texttt{Away} and \texttt{Present}. Based on these states, the Lamp-KP turns the lamps on or off. For example, when the \texttt{Present} state is specified by the P-KP, the Lamp-KP sends  the \texttt{ON} command to all lamps, and the \texttt{OFF} command when the \texttt{Away} state is specified.  The Lamp-KP is also subscribed to the states of the SLT-KP. The sequence diagram for the P-KP, SLT-KP, Lamp-KP and SIB is shown in Figure \ref{sequence-lamp}.

%When music starts rending then the Lamp-KP sends DIM command to the lamps. 

\begin{figure}
\centering
\includegraphics[width=200px]{n900}
\caption{Sequence diagram of Music Player KP subscription measurement}
\label{N900}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=200px]{sequence-lamp}
\caption{Sequence diagram of Presence-KP and Lamp-KP}
\label{sequence-lamp}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=250px]{lamp-KP}
\caption{Lamp-KP}
\label{lamp-KP}
\end{figure}

\subsubsection{Performing reasoning with the SIB}

Reasoning on information contained within the SIB was performed using OWL 2 RL and SPIN (See Section \ref{SPIN}).  OWL inferences for OWL 2 RL were executed by using SPIN rules. For more information on OWL 2 RL and SPIN, see Section \ref{owl2rl}.

When the SPIN engine is started (we use the open source TopBraid SPIN API), it iterates until there are no new triples constructed - we call this one \emph{reasoning cycle}. Existing inferences from a previous reasoning cycle are cleared before each new reasoning cycle. Triples that were previously inferred (based on a CONSTRUCT SPIN rule), are removed when the asserted triple (which caused the inferred triple) does not exist anymore. Triples are inferred (using SPIN rules) when new triples are inserted by devices connected to a triple store. For example, consider a device inserting a \texttt{sc:connectedTo} relationship when it is connected to another device. When the asserted triples are later removed by these devices (e.g. a device removing its \texttt{sc:connectedTo} relationship when the other device disconnects), the triples that were inferred based on these triples should be removed as well.

For the pilot, constraint violation checking was disabled, as this introduced quite a large delay ($>1000ms$), and was not necessary for the purposes of the pilot. Constraint checking ensures that instances in the triple store meet the constraints attached to classes and properties in an ontology. Constraint violation checks are computationally expensive and cannot be performed for each add, remove and update operation. One possible solution is to perform constraint violation checks at regular intervals and then remove the offending triples.

\subsubsection{OWL 2 RL and SPIN}
\label{owl2rl}

An example of where an OWL 2 property chain is used to perform semantic matching of media types is shown as:\\


\noindent
transmitsMediaType~\ensuremath{\circ}~isAcceptedMediaTypeOf\\~\ensuremath{\sqsubseteq}~convertsMediaType\footnote{The concatenation of two relations $R$ and $S$ is expressible by $R \circ S $, while $ R \sqsubseteq S$ indicates that $R$ is a subset of $S$ }\\


This is modelled in the ontology as:
\usemintedstyle{manni}
\begin{minted}[fontsize=\footnotesize]{turtle}
sc:convertsMediaType
	a owl:ObjectProperty; 
	a owl:IrreflexiveProperty ;
    owl:propertyChainAxiom
    	(sc:transmitsMediaType 
		 sc:isAcceptedMediaTypeOf) .
\end{minted}

% (moved to OntologyEngineering)
% We realized early on that some kind of rule-based functionality will be required in addition to the functionality provided by an ontology language like OWL 2. Originally we considered using SWRL (Semantic Web Rule Language) \cite{Niezen2011}, but it was deemed not expressive enough. Due to it being a DL (Description Logic)-safe language, it is not possible to create new individuals based on inferences. With SPARQL, this is possible using a CONSTRUCT query, defined as a SPIN rule as shown in Listing \ref{constructspin}.
% 
% 
% \begin{figure*} % \begin{listing} does not seem to support twocolumn
% % to use sparl lexer, install https://github.com/gniezen/n3pygments	
% \begin{minted}[linenos,
%                numbersep=5pt,
%                %gobble=2,
%                frame=lines,
%                fontsize=\footnotesize,
%                framesep=2mm]{sparql} 
% CONSTRUCT {
%     ?mp a sc:MediaPath .
%     ?x3 sc:hasMediaPath ?mp .
%     ?mp bonding:mediaSourceSO ?x2 .
%     ?mp bonding:mediaOriginator ?this .
% }
% WHERE {
%     ?this sc:convertsMediaType ?x2 .
%     ?x2 sc:convertsMediaType ?x3 .
%     ?this sc:connectedTo ?x3 .
%     BIND (IRI(fn:concat("example.com/ontology#mediaPath_", afn:localname(?this),
%      "_to_",afn:localname(?x3))) AS ?mp) .
% }
% \end{minted} 
% \captionof{listing}{Creating a new individual using SPARQL CONSTRUCT and SPIN} 
% % see http://tex.stackexchange.com/a/12430/8419
% \label{constructspin}
% \end{figure*}
% 
% In the example, a new \texttt{mediaPath} individual is created if two smart objects are connected to each other and there is a \texttt{mediaSourceSO} (semantic transformer) that converts the media types between them. This could be a media player transmitting music as source, an ambient lighting object that accepts RGB colour values as sink, and a semantic transformer that converts audio streams into RGB lighting information. For more information about media paths and semantic transformers, see \cite{Niezen2011}.
% 
% The \texttt{?this} variable indicates to SPIN how the definition should be applied to the members of a class, as the rule itself is defined as part of the class definition - thus defining the scope of the query. \texttt{fn:concat} and \texttt{afn:localname} are SPIN functions used to concatenate the name of the individual and retrieve the local names of the variables used respectively.

% Although SWRL is supported by the Pellet and HeRMiT reasoners, it is not yet standardized and has stayed a W3C Member Submission\footnote{http://www.w3.org/Submission/SWRL/} since 2004. SPARQL is a W3C Recommendation, actively developed and well supported by the Semantic Web community.

We made use of OWL 2 RL/RDF Rules, which is a \emph{semantic} subset of OWL 2 \emph{Full}. This should not be confused with the first part of the OWL 2 RL Profile\footnote{http://www.w3.org/TR/owl-profiles/}, which is a \emph{syntactic} subset of OWL 2 \emph{DL}, and restricted in the type of inferences that can be performed. In practice, most OWL 2 reasoners implement OWL 2 RL/RDF Rules (from here on known as OWL 2 RL). OWL 2 RL addresses a significant subset of OWL 2, including property chains and transitive properties. It is fully specified as a set of rules - in our case, as a set of SPIN rules\footnote{ http://topbraid.org/spin/owlrl-all}. This means that it is even possible to select only the parts of OWL 2 that are required for a specific ontology, to allow for scalable reasoning.
% Could add info about tractability

During the smart home pilot, all SSAP messages received by the SIB were logged for further analysis.

\subsection{Experimental Results}
\label{results}

After every reasoning cycle both the asserted and inferred models were written to disk, generating a total of 8306 models during the pilot. Reasoning was performed once, after all ontologies were loaded, and then for every add, remove and update operation. This resulted in a total of 5158 measurements of model size and reasoning time during the pilot. No reasoning was performed during queries.

During the pilot, 70655 total queries were performed by devices connected to the SIB. The time to perform each query was recorded on the SIB, and is shown in Figure \ref{querytime}. The histogram with bin size 25 is plotted on a logarithmic scale. Around 70000 queries take 2ms or less to complete, accounting for more than 99\% of the queries. Of all the queries, only 3 queries took 30ms or longer to complete, with all queries completing in less than 60ms. Keep in mind that these measurements were performed on the SIB, hence it does not take network latency into account.

\begin{figure}
\centering
\includegraphics[width=250px]{querytime}
\caption{Query time measurements on SIB}
\label{querytime}
\end{figure}

Figure \ref{kdeplot} shows the histograms, Gaussian kernel density estimates (KDEs) and cumulative distribution functions (CDFs) of the Connector KP and Sound/Light Transformer KP \emph{query time measurements}. A bin size of 20 and a bandwidth of 0.5 was used to plot the figures. It shows that the typical query time for the Connector KP is very short, with a few outliers that took a very long time to complete (35.2s). For the SLT KP, the case is similar, but there are no extreme outliers, with the longest query taking only 587ms to complete. Note that the KDE provides similar information to the histogram, but handles outliers more gracefully by not using binning, and also results in a smoother graph.

The CDF of the Connector KP indicates that queries taking more than 2 seconds to complete are very rare, but the ones that do take an unusually long time to complete. We believe that it could be related to problems in the wireless network, or related to the Python implementation of the Knowledge Processor Interface (KPI), as the problem did not present itself when using other KPI implementations.

For the SLT KP, most queries completed within 100ms, with very few queries taking longer than 500ms to complete.

\begin{figure}
\centering
\includegraphics[width=250px]{kdeplot}
\caption{Histograms, kernel density estimates and cumulative distribution functions of Connector KP and Sound/Light Transformer KP measurements}
\label{kdeplot}
\end{figure}

For the music player KP, most subscription notifications completed in an average of 0.86s, as shown in Figure \ref{n900plot}. Keep in mind that after the new PlayEvent is added, inferencing is performed on the triple store before the subscribe notification is generated. Summary statistics of Music Player KP, Connector KP and Sound/Light Transformer KP measurements are shown in Table \ref{summaryKP}.

\begin{figure}
\centering
\includegraphics[width=250px]{n900plot}
\caption{Subscription measurements of Music Player KP}
\label{n900plot}
\end{figure}

\begin{table}[!t]
\caption{Summary statistics of Music Player KP, Connector KP and Sound/Light Transformer KP measurements}
\label{summaryKP}
\centering
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Component &	 Nr. of obs. & Min. & Max. & Mean &	Std. dev.\\
\hline
Music Player KP & 264 & 0.074 & 9.975 & 0.861 & 1.017\\
Connector KP & 961 & 0.044 & 35.184 & 0.275 & 1.942\\
Sound/Light KP  & 86 & 0.06 & 0.587 & 0.131 & 0.122 \\
Lamp-KP & 98 & 0.012 & 0.049 & 0.03 &0.006 \\
Presence-KP & 172 & 0.145 & 0.244 & 0.176 & 0.018 \\
\hline
\end{tabular}
\end{table}

In Figure \ref{stats}, the following is shown:
\begin{itemize}
\item Model size: Number of triples asserted by ontology or connected KPs
\item Inferred model size: Number of triples inferred by reasoning engine
\item Inferencing duration: Time (in ms) to complete one reasoning cycle
\end{itemize}

The sharp peaks indicate the times that the SIB was restarted. The first reasoning cycle after a restart takes about 3 seconds,  with subsequent cycles taking on average 275ms (as seen in Table \ref{summarySIB}).

There is a slow but steady increase in the number of triples as new events get added to the triple store. After each restart these events are cleared and the base assertions loaded from the ontology. These assertions include the OWL 2 RL specification, stored as SPIN rules, which account for the large number of triples.

\begin{figure}
\centering
\includegraphics[width=250px]{stats}
\caption{Size of asserted and inferred models for each iteration, including reasoning time}
\label{stats}
\end{figure}

\begin{table}[!t]
\caption{Summary statistics for asserted and inferred model sizes and reasoning time}
\label{summarySIB}
\centering
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Component &	 Nr. of obs. & Min. & Max. & Mean &	Std. dev.\\
\hline
Model size	& 5158 & 1346 &	3396 &	2916.7 & 201.07 \\
Inferred model size &	5158 &	1369 &	1819 &	1501.8 & 107.6 \\
Reasoning time & 5158 &	181 & 2912 & 274.99 & 152.96 \\
\hline
\end{tabular}
\end{table}

In Figure \ref{CPD}, the measurements show that the delay between the P-KP and the SIB is rather large with a considerable variance. The communication from the P-KP to the SIB consists of an update request and the related confirmation response. The average delay of 176.71 milliseconds is the largest component of the total end-to-end delay between the links. The communication from the SIB to the Lamp-KP consists of an indication event from the SIB due to a subscription and results in a query request from the Lamp-KP, terminated by a confirmation response by the SIB. There is some variance in the communication delay and the average delay of 25.87 milliseconds might become problematic when the SIB has to inform and handle multiple subscribers.

\begin{figure}
\centering
\includegraphics[width=250px]{CPD}
\caption{Cummulitive probability distribution of delays between Presence-KP and SIB, and SIB and Lamp-KP}
\label{CPD}
\end{figure}

\subsection{Discussion}

During the pilot, most problems could be attributed to problems with the wireless network. A number of devices from different manufacturers experienced intermittent problems while connected to the SIB. For the purposes of the pilot, these devices were then connected to the SIB via ethernet. This does, however, demonstrate some of the problems with existing wireless networking technologies. It cannot be expected that a device with a wireless connection will always stay connected to the smart space, even when it is within range of the wireless router.

%During one stakeholder visit, the comment was made that what we term semantic transformers, could also be termed transcoders, as they transcode data from one source type to another. With a large number of stakeholder visits the similarities of the SOFIA system to Universal Plug \& Play (UPnP) were discussed. It was pointed out that uPnP only covers one domain, and that it does not allow for the same type of device and capability descriptions on the semantic level.

If we compare our query time measurements to the ones performed on Smart-M3 (4.4ms) and RIBS (0.65ms), we can see that the KPs were substantially slower: The Connector KP at 44ms, the Music Player KP at 74ms and the Sound/Light KP at 60ms. This can be attributed to additional network latency in the field study that approximated a real-world environment. Query measurements that were performed on the ADK-SIB directly (0.445ms) shows that it performs even better than RIBS, but this is not directly comparable as it does not include network latency time. If network latency is taken into account, measured at 0.43ms per packet round-trip by \cite{Etelapera2011}, RIBS is still faster.

The subscription indication measurements of our setup are also significantly slower. While the Smart-M3 measurement was only 140ms, the Music Player KP measures around 860ms. This is mostly due to the additional time required for reasoning, which on average takes about 275ms. Other contributing factors include the number of devices used, the number of triples and the network environment.

In the 1960s there already existed some controversy over the maximum allowable response times in human-computer interfaces \cite{Miller1968}. It was shown that different human actions will have different acceptable response times. While the delay between pressing a key and visual feedback should be no more than 0.1-0.3 seconds, the response to a inquiry request may take up to 2 seconds. The performance measurements are still well within this two-second limit, indicating that from a user's point of view, when performing routine tasks, the system is responsive enough. The user study and interviews performed during the pilot seem to confirm this, as no participant indicated any issues with regards to the responsiveness of the system.

The scalability of the system should still be evaluated with larger triple sizes, but we do not foresee any scalability issues, due to the platform and software architecture used. 

%When addressing another human being, we expect some communicative response between two and four seconds - any longer delay breaks the thread of communication and becomes embarrassing.


%SeNAmI end


\section{Evaluating the ontology}

A variety of methods have been proposed \cite{DAquin2011} to evaluate ontology quality:

\begin{itemize}
	\item Testing and assessing the formal properties, e.g. using OntoClean (see Section TODO)
	\item Using ontology design patterns (see Section \ref{DesignPatterns})
	\item Using unit tests as employed in software engineering
	\item Using opinions and rating using crowd-sourcing
	\item Using automatically generated metrics
\end{itemize}

In the book Beautiful Data \cite{Segaran2009}, the notion of beauty is described as ``a simple and elegant solution to some kind of problem''. In a paper on the notion of beauty when building and and evaluating ontologies, D'Aquin and Gangemi \cite{DAquin2011} argue that the GoodRelations\footnote{http://www.heppnetz.de/projects/goodrelations/} e-commerce ontology could be considered an example of a beautiful ontology. It addresses a complex domain and covers many of the complex situations that can occur in the domain. It is well designed, ontologically consistent, lightweight and used extensively by practitioners in the domain. GoodRelations is an OWL 1 DL ontology that is used by stores to describe products and their prices and features. Companies using the ontology include, Google, BestBuy, Sears, K-Mart and Yahoo.

Hepp \cite{Hepp2007} describes a number of characteristics that can be used to evaluate an ontology:

\begin{itemize}
	\item Expressiveness: An ontology of higher expressiveness would be richly axiomatised in higher order logic, while a simple vocabulary would be of low expressiveness.
	\item Size of community: As ontologies are considered community contracts (see Section \ref{CommunityContract}), an ontology targeted towards a large community should be easy to understand, well documented and of a reasonable size.
	\item Number of conceptual elements: A larger ontology is more difficult to visualise and review. A reasoner could also take a long time to converge when the ontology is very large.
	\item Degree of subjectivity: This is very much related to the domain of the ontology, where something like religion would be more subjectively judged than engineering.
	\item Average size of specification per element: The number of axioms or attributes used to describe each concept influences the ontological commitment\label{OntologicalCommitment} that must be made before adopting the ontology. \marginpar{Ontological commitment relates to \emph{premature commitment}, which is described in more detail in Section TODO on the Cognitive Dimensions framework.}
\end{itemize}


\section{Validating the work using the Cognitive Dimensions framework}
\label{CognitiveDimensions}

The Cognitive Dimensions (CD) framework is a broad-brush approach to evaluating the usability of interactive devices and non-interactive notations, e.g. programming languages and APIs. It establishes a vocabulary of terms to describe the structure of an artefact and shows how these terms can be traded off against each other.These terms are (in principle) mutually orthogonal.
Traditional HCI evaluation techniques focus on 'simple tasks' like deleting a word in a text editor, or trying to determine the time required to perform a certain task. They are not well suited to evaluating programming environments or notational issues. The CD framework has been used to perform usability analyses of visual programming environments \cite{Green1996} as well as APIs \cite{Clarke2004}. \cite{Mavrommati2004} used the framework to evaluate the usability of an editing tool that is used to manage device associations in a home environment.

For our evaluation, we focused on a subset of cognitive dimensions that are related to ubicomp ontologies and systems. What follows is a list of these dimensions, including a short description (where necessary) as well as an example question.


\begin{description}
  \item[Levels of abstraction] 	\label{cdDef}
	An abstraction is a grouping of elements that is treated as one entity, either for convenience or to change the conceptual structure \cite{Green1996}. \emph{What are the minimum and maximum levels of abstractions?}
    \item[Closeness of mapping] \emph{How clearly did the available components map onto the problem domain?}
	\item[Consistency] \emph{When some of the language has been learnt, how much of the rest can be inferred by the developer? Where there were concepts in the ontology that mean similar things, is the similarity clear from the way they appear?}
	\item[Viscosity] To solve problems of viscosity, usually more abstractions (see earlier definition) are introduced in order to handle a number of components as one group, for example in object-oriented programming \cite{Green1996}. An example of viscosity is where it is necessary to make a global change by hand because the environment used does not have a global update tool. \emph{How much effort was required to make a change to the environment?}
	\item[Role expressiveness]
		The dimension of role expressiveness is intended to describe how easy it is determine what a certain part is for. \emph{Are there parts of the ontology that are particularly difficult to interpret?}
	\item[Hard mental operations] \emph{Are there places where the developer needs to resort to fingers or pencilled annotation to keep track of what is happening? What kind of things required the most mental effort with this system and ontology?}
	\item[Error-proneness] \emph{Did some kinds of mistakes seem particularly common or easy to make?}
	\item[Hidden dependencies] \emph{If some parts were closely related to other parts, and changes to one may affect the other, are those dependencies visible? }
\end{description}


 
% \subsection{Progressive evaluation}
% \label{progressive}
% For novices, it should be easy to check a program fragment before adding to it \cite{Green1996}. A partially-complete program should have the ability to be executed to obtain feedback on whether the developer has implemented new functionality correctly.


\section{Related work}
	Microsoft \cite{Clarke2004} used the CD framework to evaluate API usability, as part of a user-centered design approach to API design. Every API has a set of actions that it performs. However, developers browsing the API might not comprehend all the possible actions that the API offers. In a usability study they asked a group of developers to use an API to perform a set of tasks, and then asked a set of questions for each dimension. For example, for role expressiveness(see Section \ref{cdDef}), the question was posed that when reading code that uses the API, if it was easy to tell what each section of the code does and why.

\section{Evaluating ontologies}
    
Every notation highlights some kinds of information at the expense of obscuring other kinds \cite{Green1996}. For example, rule-based languages highlight conditions under which certain actions are taken, while it hides the sequential ordering of these actions. There must also be a cognitive fit between the mental representation and external representation. For example, if your mental representation is in control flow form, a data flow language will be difficult to use. 




\section{Method}

The original Cognitive Dimensions questionnaire \cite{Blackwell2007} was adapted for use with ubicomp ontologies. Non-relevant questions were eliminated and some wording and questions were adjusted to the subject matter, without changing the fundamental meaning of the questions themselves. 

Developers of the SOFIA Smart Home pilot completed the questionnaire, as well as students and developers affiliated to the University of Bologna.

\begin{figure}[bth]
        \subfloat[Background of correspondents]
        {\includegraphics[width=.45\linewidth]{background}} \quad
        \subfloat[Correspondents' experience with software engineering and programming]
        {\label{experience}%
         \includegraphics[width=.45\linewidth]{experience}} \\
        \subfloat[Correspondents' experience with ontologies and semantic technologies]
        {\includegraphics[width=.45\linewidth]{expOntologies}} \quad
        
        \caption{Correspondent demographics}\label{demographics}
\end{figure}

% \begin{figure}
%         \includegraphics[width=250px]{background}
%         \caption{Background of correspondents}
% 		\label{background}
% \end{figure}
% 
% \begin{figure}[bth]
%         \includegraphics[width=.45\linewidth]{experience}
%         \caption{Correspondents' experience with software engineering and programming}
% 		\label{experience}
% \end{figure}
% 
% 
% \begin{figure}[bth]
%         \includegraphics[width=.45\linewidth]{expOntologies}
%         \caption{Correspondents' experience with ontologies and semantic technologies}
% 		\label{expOntologies}
% \end{figure}

\section{Results}


\subsection{Levels of abstraction}

\begin{quote}
	\emph{Were you able to define your own concepts and terms using the system and ontology? Did you make use of different levels of abstraction?} An abstraction is a grouping of elements to be treated as one entity. In the ontology, these are defined as superclasses and subclasses, e.g. PlayEvent is a subclass of MediaPlayerEvent. Please indicate to what extent you made use of different levels of abstraction. If you did not use it, please indicate why.
\end{quote}

Most developers were able to make use of the existing concepts as defined, where the definition included different abstraction levels. Where necessary, developers were able to define their own concepts using different abstraction levels. Some of the developers used a simple ontology that did not require different levels of abstraction. As the level of knowledge about ontologies differed between different parties working on the same project, this necessitated the simplification of the ontology to a schema without semantics in some cases. Others avoided ontological reasoning altogether by embedding the logic in the KP itself.


\subsection{Closeness of mapping}
\label{resultsCloseness}

\begin{quote}
	\emph{How clearly did the available components map to the problem domain?}
	Did you have to define any of your own components, or were any special tricks needed to accomplish specific functionality?
\end{quote}


Most of the developers experienced a clear, consistent mapping, with the domain mapped to already available components. In a few cases, developers developed their own components.

While it was easy to achieve the required functionality, it remains difficult to achieve component re-use. This becomes problematic for achieving emergent intelligent behaviour. 
It was also stated that more detailed descriptions of device capabilities, for example the coverage area of a presence sensor, are required.


\subsection{Consistency}

\begin{quote}
	\emph{Where there were concepts in the ontology that mean similar things, is the similarity clear from the way they appear?}
	Are there places where some things ought to be similar, but the ontology defines them differently? Please give examples.
\end{quote}

Most developers thought that similar entities in the ontology were subclassed correctly. It was indicated that \texttt{owl:sameAs} may be useful to indicate that different terms with the same meaning are in fact the same thing.

Developers not well acquainted with ontologies found it difficult to understand the difference between declaring entities using \texttt{rdfs:subClassOf} and declaring them as \emph{individuals} or instances, as well as how to model an entity that \emph{contains} another entity.

Similar entities were not always instantiated in the same way, for example no state information was available for some smart objects.
Where multiple domain ontologies with similar concepts were used, these concepts were not aligned - most developers expected some upper (or core) ontology to align and unify main concepts.
Concepts need a clear textual description and usage examples to make them easier to understand.

\subsection{Viscosity}

\begin{quote}
	\emph{How much effort was required to make a change to the environment? Why?}
	How difficult is it to make changes to your program, the ontology or the system? For example, was it necessary to make a global change by hand because no global update tools were available?
\end{quote}

Although different domains used different terms to define ontological concepts, most developers found it quite easy to make changes for ontological agreement. However, changes to the ontology sometimes necessitated changes at code level. In most cases, it was easier to adapt to changes on a semantic level, as the KP domain boundaries were well defined.

Using ontologies made it easier to allow for definition changes at run-time. Depending on the inferencing method used, changes to the ontology could require some existing inferences to be removed. 

Embedded system developers found changes to be more difficult to implement, as it still required rebuilding images and downloading them to embedded boards for each modification. Some found it difficult to view changes made to the environment, due to a lack of tools to explore the contents of the SIB.


\subsection{Role expressiveness}

\begin{quote}
	\emph{Are there parts of the ontology that are particularly difficult to interpret?}
	How easy is it to answer the question: 'What is this bit for?' Which parts are difficult to interpret?
\end{quote}

Most responses indicated that ontology was easy to understand. More clarifying comments inside ontology could be useful - this can be implemented using the \texttt{rdfs:comment} field. Some developers indicated that application ontologies (ontologies that are device-specific) were still hard to interpret. 

Some concepts might be instinctively interpreted differently, but the defined meaning became clear when viewed in context with the rest of the ontology. The ontology provides the structure that is necessary to make sense of the concepts.


\subsection{Hard mental operations}

\begin{quote}
	\emph{What kind of things required the most mental effort with this system and ontology?}
	Did some things seem especially complex or difficult to work out in your head (e.g. when combining several things)? What are they?
\end{quote}

Multiple developers indicated that ontologies are not an easy concept to grasp and that common practice is not always clear. Once familiar with ontologies, and understanding the specific ontologies involved, developers thought they were easy to use.  

There is still effort required to make the system adaptive. This issue is also mentioned in Section \ref{resultsCloseness}.


\subsection{Error-proneness}

\begin{quote}
	\emph{Did some kinds of mistakes seem particularly common or easy to make? Which ones?}
	Did you often find yourself making small slips that irritate you or make you feel stupid? What are some examples?
\end{quote}

Some developers found it quite easy to mistype strings literals, indicating that it would be better to define entities to represent strings that occur more than once. Mistyping URIs was another common error, as well as mixing up namespaces. Developers not familiar with ontologies also mixed up or misunderstood the differences between URIs and literals.

\subsection{Hidden dependencies}

\begin{quote}
	\emph{If some parts were closely related to other parts, and changes to one may affect the other, are those dependencies visible?}
	What kinds of dependencies were hidden? How difficult was it to test the implemented system? Were there hidden faults that were difficult to find?
\end{quote}

Multiple developers noted that using ontologies made the relationships among entities/parts more visible. 

Changes to the ontology may affect others, therefore versioning and update notification are important. In the system used no errors were raised when components adhered to different versions of the ontology. Broken dependencies were only visible when the overall system failed.

One developer indicated that the publish/subscribe approach followed by the system architecture allowed for minimal dependencies due to loose coupling.

Some developers noted that it was difficult to determine when something did not work. This was especially noticeable in the case of subscriptions, were it becomes really difficult to understand why a certain subscription-based notification was not received.

One developer suggested that an ontology viewer could be used to make dependencies more visible, and that Prot\'eg\'e\footnote{http://protege.stanford.edu} ontology editor is too complex for most users.





\subsection{Non-CD related questions}

Some questions in the questionnaire was not directly related to the cognitive dimensions, but were meant to elicit more general responses to the usability of the ontologies and system.

\subsubsection{What obstacles made it difficult to use the system?}

This question was based on a survey done by \cite{Robillard2009} to determine aspects that make APIs hard to learn. The question was phrased as follows:

\begin{quote}
\emph{What obstacles made it difficult for you to use the system? }Obstacles can have to do with the system itself, with your background, with learning resources etc. List the three most important obstacles, in order of importance (1 being the biggest obstacle). Please be more specific than the general categories mentioned here.
\end{quote}

Responses included:

\begin{itemize}
	\item Lack of background in ontologies
	\item Poor documentation
	\item Insufficient code examples
	\item SSAP poorly documented
	\item Difficult setup and installation procedure
	\item Lack of proper tools for viewing and exploring contents of SIB
	\item Reliability of subscription mechanism, especially on wireless networks
	\item QNames not supported on all platforms, requiring full URI to be specified
	\item Ontology agreement
	\item Stability, performance, network issues
\end{itemize}




\subsubsection{What did you appreciate most about the system and ontology?}

\begin{itemize}
	\item Decoupling of interaction between components (i.e. all communication through broker, but could be single point of failure)
	\item Semantic interoperability between different devices, manufacturers and architectures
	\item Easy and quick to define new applications based on ontologies (after training)
	\item Using semantic connections to connect devices
	\item Ontology usable in different domains and more complex scenarios
	\item Ability to react to context changes through subscriptions
\end{itemize}


\begin{quote}
Once you have agreed on the ontologies and the KP's functionalities, you can focus on handling the various subscriptions and inserting the necessary triples. One does not have to focus on the communication protocols used or on the communication with the other components themselves.
\end{quote}

\subsubsection{Can you think of ways the design of the system and ontology can be improved?}
\begin{itemize}

	\item SIB discovery
	\item Agreement on ontological concepts to be used by a technical group
	\item Tools for ontology design (currently external tools are required)
	\item Better documentation
	\item Self-description of UI concepts and component functionality 
	\item Hierarchic Smart Spaces
	\item Locality/routing/separation of message buses
	\item Authentication/security/locking
	\item Forcing a programming paradigm like OOP influences semantic treatment
	\item How to handle faulty smart objects
\end{itemize}


\section{Discussion}

Ontologies allow developers to create additional levels of abstraction when the existing abstractions are not sufficient. The bigger issue seems to be unfamiliarity with ontologies, with some developers going so far as to embed all logic in the code itself (in order to avoid using ontologies).

For the ontologies we defined, there seems to be clear mapping between objects in the domain and the ontological entities that they are mapped to. Adding additional components where necessary did not present any problems. One area that needs more attention is the extending the level of detail for device capability descriptions. Keep in mind, however, that too many low-level primitives create a cognitive barrier to programming \cite{Green1996}. It is not easy to deal with entities in the program domain that do not have corresponding entities in the problem domain. For example, having many ways to describe a presence sensor, when only one or two of these are relevant to the problem domain, makes it more difficult for the developer to comprehend.

When creating an ontology, it is important to provide clear textual descriptions, clarifying comments and usage examples for concepts to make them easier to understand. The GoodRelations\footnote{http://purl.org/goodrelations/v1} e-commerce ontology is a good example of how this can be achieved.

Tools to explore the contents of the triple store more efficiently could decrease viscosity, as it would simplify viewing changes made to the environment and make dependencies more visible. Existing ontology viewers are still considered complex to use, usually only by ontology experts. Tools to automatically detect namespaces and prevent mistyping of URIs and strings used in the ontology would also be very useful.

There is still effort required to make the system adaptive in order to achieve emerging behavior. There is not enough component re-use, which may be due to lack of knowledge of existing components and their behavior. Existing components should be easy to repurpose and shared behavior must be properly documented. 








TODO Compare with DLNA/UPnP as a case study

Possible TODO
Compare to Logitech Harmony One Universal Remote Control (See Evernote)
- Compare ontological approach to APIs http://itu.dk/stud/speciale/gamelib/APIUsability.pdf
- Validate generalization of ontology (compared to existing ones)